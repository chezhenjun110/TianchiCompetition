{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "FUDGE_FACTOR = 0.985  # Multiply forecasts by this\n",
    "XGB_WEIGHT = 0.3200\n",
    "BASELINE_WEIGHT = 0.0100\n",
    "OLS_WEIGHT = 0.0620\n",
    "NN_WEIGHT = 0.0800\n",
    "CAT_WEIGHT=0.4000\n",
    "XGB1_WEIGHT = 0.8000  # Weight of first in combination of two XGB models\n",
    "BASELINE_PRED = 5.631925   # Baseline based on mean of training data, per Oleg\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "import datetime as dt\n",
    "from dateutil.parser import parse\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "from dateutil.parser import parse\n",
    "###### READ IN RAW DATA\n",
    "#\n",
    "print( \"\\nReading data from disk ...\")\n",
    "data_path = 'datas/'\n",
    "train = pd.read_csv(data_path+'train.csv',encoding='gb18030')\n",
    "test = pd.read_csv(data_path+'testA.csv',encoding='gb18030')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cfaa16a482ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_feat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_feat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmake_feat_lightgbm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'血糖'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "#################\n",
    "#################\n",
    "###  LightGBM  ##\n",
    "#################\n",
    "#################\n",
    "#\n",
    "##### PROCESS DATA FOR LIGHTGBM\n",
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "def make_feat_lightgbm(train,test):\n",
    "    train_id = train.id.values.copy()\n",
    "    test_id = test.id.values.copy()\n",
    "    data = pd.concat([train,test])\n",
    "    data['性别'] = data['性别'].map({'男':1,'女':0})\n",
    "    data['体检日期'] = (pd.to_datetime(data['体检日期']) - parse('2016-10-09')).dt.days\n",
    "    data.fillna(data.median(axis=0),inplace=True)\n",
    "\n",
    "    for c, dtype in zip(data.columns, data.dtypes):\n",
    "\n",
    "        if dtype == np.float64:\n",
    "\n",
    "            data[c] = data[c].astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    train_feat = data[data.id.isin(train_id)]\n",
    "\n",
    "    test_feat = data[data.id.isin(test_id)]\n",
    "\n",
    "    \n",
    "\n",
    "    return train_feat,test_feat\n",
    "\n",
    "df_train,df_test= make_feat_lightgbm(train,test)\n",
    "\n",
    "x_train = df_train.drop(['id', '血糖'], axis=1)\n",
    "\n",
    "y_train = df_train['血糖'].values\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "del df_train; gc.collect()\n",
    "\n",
    "x_train = x_train.values.astype(np.float32, copy=False)\n",
    "\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "##### RUN LIGHTGBM\n",
    "\n",
    "params = {}\n",
    "\n",
    "params['max_bin'] = 10\n",
    "\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "\n",
    "params['boosting_type'] = 'gbdt'\n",
    "\n",
    "params['objective'] = 'regression'\n",
    "\n",
    "params['metric'] = 'mse'          # or 'mae'\n",
    "\n",
    "params['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\n",
    "\n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "\n",
    "params['bagging_freq'] = 40\n",
    "\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "\n",
    "params['verbose'] = 0\n",
    "\n",
    "params['feature_fraction_seed'] = 2\n",
    "\n",
    "params['bagging_seed'] = 3\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "print(\"\\nFitting LightGBM model ...\")\n",
    "\n",
    "clf = lgb.train(params, d_train, 1000)\n",
    "\n",
    "del d_train; gc.collect()\n",
    "\n",
    "del x_train; gc.collect()\n",
    "\n",
    "print(\"\\nPrepare for LightGBM prediction ...\")\n",
    "\n",
    "print(\"   Preparing x_test...\")\n",
    "\n",
    "x_test = df_test.drop(['id','血糖'], axis=1)\n",
    "\n",
    "print(\"\\nStart LightGBM prediction ...\")\n",
    "\n",
    "p_test = clf.predict(x_test)\n",
    "\n",
    "del x_test; gc.collect()\n",
    "\n",
    "print( \"\\nUnadjusted LightGBM predictions:\" )\n",
    "\n",
    "print( pd.DataFrame(p_test).head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "\n",
    "################\n",
    "\n",
    "##  XGBoost   ##\n",
    "\n",
    "################\n",
    "\n",
    "################\n",
    "\n",
    "#### PROCESS DATA FOR XGBOOST\n",
    "\n",
    "print( \"\\nProcessing data for XGBoost ...\")\n",
    "\n",
    "data_path = 'datas/'\n",
    "\n",
    "train = pd.read_csv(data_path+'train.csv',encoding='gb18030')\n",
    "\n",
    "test = pd.read_csv(data_path+'testA.csv',encoding='gb18030')\n",
    "\n",
    "def make_feat_xgb(train,test):\n",
    "\n",
    "    train_id = train.id.values.copy()\n",
    "\n",
    "    test_id = test.id.values.copy()\n",
    "\n",
    "    data = pd.concat([train,test])\n",
    "\n",
    "    data['性别'] = data['性别'].map({'男':1,'女':0})\n",
    "\n",
    "    data['体检日期'] = (pd.to_datetime(data['体检日期']) - parse('2016-10-09')).dt.days\n",
    "\n",
    "    \n",
    "\n",
    "    data.fillna(-1,axis=1,inplace=True)\n",
    "\n",
    "    for c, dtype in zip(data.columns, data.dtypes):\n",
    "\n",
    "        if dtype == np.float64:\n",
    "\n",
    "            data[c] = data[c].astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    train_feat = data[data.id.isin(train_id)]\n",
    "\n",
    "    test_feat = data[data.id.isin(test_id)]\n",
    "\n",
    "    \n",
    "\n",
    "    return train_feat,test_feat\n",
    "\n",
    "df_train,df_test= make_feat_xgb(train,test)\n",
    "\n",
    "y_train = df_train['血糖'].values\n",
    "\n",
    "x_train = df_train.drop(['id', '血糖'], axis=1)\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "x_test = df_test.drop(['id','血糖'], axis=1)\n",
    "\n",
    "# shape        \n",
    "\n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "##### RUN XGBOOST\n",
    "\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "\n",
    "# xgboost params\n",
    "\n",
    "xgb_params = {\n",
    "\n",
    "    'eta': 0.037,\n",
    "\n",
    "    'max_depth': 5,\n",
    "\n",
    "    'subsample': 0.80,\n",
    "\n",
    "    'objective': 'reg:linear',\n",
    "\n",
    "    'eval_metric': 'rmse',\n",
    "\n",
    "    'lambda': 0.8,   \n",
    "\n",
    "    'alpha': 0.4, \n",
    "\n",
    "    'base_score': y_mean,\n",
    "\n",
    "    'silent': 1\n",
    "\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "num_boost_rounds = 250\n",
    "\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "# train model\n",
    "\n",
    "print( \"\\nTraining XGBoost ...\")\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost ...\")\n",
    "\n",
    "xgb_pred1 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nFirst XGBoost predictions:\" )\n",
    "\n",
    "print( pd.DataFrame(xgb_pred1).head() )\n",
    "\n",
    "##### RUN XGBOOST AGAIN\n",
    "\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "\n",
    "# xgboost params\n",
    "\n",
    "xgb_params = {\n",
    "\n",
    "    'eta': 0.033,\n",
    "\n",
    "    'max_depth': 6,\n",
    "\n",
    "    'subsample': 0.80,\n",
    "\n",
    "    'objective': 'reg:linear',\n",
    "\n",
    "    'eval_metric': 'rmse',\n",
    "\n",
    "    'base_score': y_mean,\n",
    "\n",
    "    'silent': 1\n",
    "\n",
    "}\n",
    "\n",
    "num_boost_rounds = 150\n",
    "\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "print( \"\\nTraining XGBoost again ...\")\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost again ...\")\n",
    "\n",
    "xgb_pred2 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nSecond XGBoost predictions:\" )\n",
    "\n",
    "print( pd.DataFrame(xgb_pred2).head() )\n",
    "\n",
    "##### COMBINE XGBOOST RESULTS\n",
    "\n",
    "xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n",
    "\n",
    "#xgb_pred = xgb_pred1\n",
    "\n",
    "print( \"\\nCombined XGBoost predictions:\" )\n",
    "\n",
    "print( pd.DataFrame(xgb_pred).head() )\n",
    "\n",
    "del df_train\n",
    "\n",
    "del x_train\n",
    "\n",
    "del x_test\n",
    "\n",
    "del dtest\n",
    "\n",
    "del dtrain\n",
    "\n",
    "del xgb_pred1\n",
    "\n",
    "del xgb_pred2 \n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "######################\n",
    "\n",
    "######################\n",
    "\n",
    "##  Neural Network  ##\n",
    "\n",
    "######################\n",
    "\n",
    "######################\n",
    "\n",
    "# Neural network copied from this script:\n",
    "\n",
    "# Read in data for neural network\n",
    "\n",
    "print( \"\\n\\nProcessing data for Neural Network ...\")\n",
    "\n",
    "data_path = 'datas/'\n",
    "\n",
    "train = pd.read_csv(data_path+'train.csv',encoding='gb18030')\n",
    "\n",
    "test = pd.read_csv(data_path+'testA.csv',encoding='gb18030')\n",
    "\n",
    "def make_feat_nn(train,test):\n",
    "\n",
    "    train_id = train.id.values.copy()\n",
    "\n",
    "    test_id = test.id.values.copy()\n",
    "\n",
    "    data = pd.concat([train,test])\n",
    "\n",
    "    data['性别'] = data['性别'].map({'男':1,'女':0})\n",
    "\n",
    "    data['体检日期'] = (pd.to_datetime(data['体检日期']) - parse('2016-10-09')).dt.days\n",
    "\n",
    "    \n",
    "\n",
    "    data.fillna(-1,axis=1,inplace=True)\n",
    "\n",
    "    for c, dtype in zip(data.columns, data.dtypes):\n",
    "\n",
    "        if dtype == np.float64:\n",
    "\n",
    "            data[c] = data[c].astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    train_feat = data[data.id.isin(train_id)]\n",
    "\n",
    "    test_feat = data[data.id.isin(test_id)]\n",
    "\n",
    "    \n",
    "\n",
    "    return train_feat,test_feat\n",
    "\n",
    "df_train,df_test= make_feat_nn(train,test)\n",
    "\n",
    "y_train = df_train['血糖'].values\n",
    "\n",
    "x_train = df_train.drop(['id', '血糖'], axis=1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "x_test = df_test.drop(['id','血糖'], axis=1)\n",
    "\n",
    "# shape        \n",
    "\n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "print(\"\\nPreprocessing neural network data...\")\n",
    "\n",
    "imputer= Imputer()\n",
    "\n",
    "imputer.fit(x_train.iloc[:, :])\n",
    "\n",
    "x_train = imputer.transform(x_train.iloc[:, :])\n",
    "\n",
    "imputer.fit(x_test.iloc[:, :])\n",
    "\n",
    "x_test = imputer.transform(x_test.iloc[:, :])\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "x_train = sc.fit_transform(x_train)\n",
    "\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "len_x=int(x_train.shape[1])\n",
    "\n",
    "print(\"len_x is:\",len_x)\n",
    "\n",
    "# Neural Network\n",
    "\n",
    "print(\"\\nSetting up neural network model...\")\n",
    "\n",
    "nn = Sequential()\n",
    "\n",
    "nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\n",
    "\n",
    "nn.add(PReLU())\n",
    "\n",
    "nn.add(Dropout(.4))\n",
    "\n",
    "nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "\n",
    "nn.add(PReLU())\n",
    "\n",
    "nn.add(BatchNormalization())\n",
    "\n",
    "nn.add(Dropout(.6))\n",
    "\n",
    "nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "\n",
    "nn.add(PReLU())\n",
    "\n",
    "nn.add(BatchNormalization())\n",
    "\n",
    "nn.add(Dropout(.5))\n",
    "\n",
    "nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "\n",
    "nn.add(PReLU())\n",
    "\n",
    "nn.add(BatchNormalization())\n",
    "\n",
    "nn.add(Dropout(.6))\n",
    "\n",
    "nn.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "\n",
    "print(\"\\nFitting neural network model...\")\n",
    "\n",
    "nn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 70, verbose=2)\n",
    "\n",
    "print(\"\\nPredicting with neural network model...\")\n",
    "\n",
    "#print(\"x_test.shape:\",x_test.shape)\n",
    "\n",
    "y_pred_ann = nn.predict(x_test)\n",
    "\n",
    "print( \"\\nPreparing results for write...\" )\n",
    "\n",
    "nn_pred = y_pred_ann.flatten()\n",
    "\n",
    "print( \"Type of nn_pred is \", type(nn_pred) )\n",
    "\n",
    "print( \"Shape of nn_pred is \", nn_pred.shape )\n",
    "\n",
    "print( \"\\nNeural Network predictions:\" )\n",
    "\n",
    "print( pd.DataFrame(nn_pred).head() )\n",
    "\n",
    "# Cleanup\n",
    "\n",
    "del train\n",
    "\n",
    "del x_train\n",
    "\n",
    "del x_test\n",
    "\n",
    "del df_train\n",
    "\n",
    "del df_test\n",
    "\n",
    "del y_pred_ann\n",
    "\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "\n",
    "################\n",
    "\n",
    "## catboost   ##\n",
    "\n",
    "################\n",
    "\n",
    "################\n",
    "\n",
    "data_path = 'datas/'\n",
    "\n",
    "train = pd.read_csv(data_path+'train.csv',encoding='gb18030')\n",
    "\n",
    "test = pd.read_csv(data_path+'testA.csv',encoding='gb18030')\n",
    "\n",
    "def make_feat_cat(train,test):\n",
    "\n",
    "    train_id = train.id.values.copy()\n",
    "\n",
    "    test_id = test.id.values.copy()\n",
    "\n",
    "    data = pd.concat([train,test])\n",
    "\n",
    "    data['性别'] = data['性别'].map({'男':1,'女':0})\n",
    "\n",
    "    data['体检日期'] = (pd.to_datetime(data['体检日期']) - parse('2016-10-09')).dt.days\n",
    "\n",
    "    \n",
    "\n",
    "#    data.fillna(data.median(axis=0),inplace=True)\n",
    "\n",
    "    train_feat = data[data.id.isin(train_id)]\n",
    "\n",
    "    test_feat = data[data.id.isin(test_id)]\n",
    "\n",
    "    \n",
    "\n",
    "    return train_feat,test_feat\n",
    "\n",
    "train_df,test_df = make_feat_cat(train,test)\n",
    "\n",
    "print('Remove missing data fields ...')\n",
    "\n",
    "missing_perc_thresh = 0.98\n",
    "\n",
    "exclude_missing = []\n",
    "\n",
    "num_rows = train_df.shape[0]\n",
    "\n",
    "for c in train_df.columns:\n",
    "\n",
    "    num_missing = train_df[c].isnull().sum()\n",
    "\n",
    "    if num_missing == 0:\n",
    "\n",
    "        continue\n",
    "\n",
    "    missing_frac = num_missing / float(num_rows)\n",
    "\n",
    "    if missing_frac > missing_perc_thresh:\n",
    "\n",
    "        exclude_missing.append(c)\n",
    "\n",
    "print(\"We exclude: %s\" % len(exclude_missing))\n",
    "\n",
    "del num_rows, missing_perc_thresh\n",
    "\n",
    "gc.collect();\n",
    "\n",
    "print (\"Remove features with one unique value !!\")\n",
    "\n",
    "exclude_unique = []\n",
    "\n",
    "for c in train_df.columns:\n",
    "\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "\n",
    "    if train_df[c].isnull().sum() != 0:\n",
    "\n",
    "        num_uniques -= 1\n",
    "\n",
    "    if num_uniques == 1:\n",
    "\n",
    "        exclude_unique.append(c)\n",
    "\n",
    "print(\"We exclude: %s\" % len(exclude_unique))\n",
    "\n",
    "print (\"Define training features !!\")\n",
    "\n",
    "exclude_other = ['id','血糖','乙肝表面抗原', '乙肝表面抗体', '乙肝e抗原', '乙肝e抗体', '乙肝核心抗体']\n",
    "\n",
    "train_features = []\n",
    "\n",
    "for c in train_df.columns:\n",
    "\n",
    "    if c not in exclude_missing \\\n",
    "\n",
    "       and c not in exclude_other and c not in exclude_unique:\n",
    "\n",
    "        train_features.append(c)\n",
    "\n",
    "print(\"We use these for training: %s\" % len(train_features))\n",
    "\n",
    "print (\"Define categorial features !!\")\n",
    "\n",
    "cat_feature_inds = []\n",
    "\n",
    "cat_unique_thresh = 10\n",
    "\n",
    "for i, c in enumerate(train_features):\n",
    "\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "\n",
    "    if num_uniques < cat_unique_thresh:\n",
    "\n",
    "        cat_feature_inds.append(i)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])\n",
    "\n",
    "print (\"Replacing NaN values by -999 !!\")\n",
    "\n",
    "train_df.fillna(-999, inplace=True)\n",
    "\n",
    "test_df.fillna(-999, inplace=True)\n",
    "\n",
    "print (\"Training time !!\")\n",
    "\n",
    "X_train = train_df[train_features]\n",
    "\n",
    "y_train = train_df['血糖']\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "X_test = test_df[train_features]\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "num_ensembles = 5\n",
    "\n",
    "y_pred_cat = 0.0\n",
    "\n",
    "for i in tqdm(range(num_ensembles)):\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "\n",
    "        iterations=1000, learning_rate=0.03,\n",
    "\n",
    "        depth=6, l2_leaf_reg=3, \n",
    "\n",
    "        loss_function='RMSE',\n",
    "\n",
    "        eval_metric='RMSE',\n",
    "\n",
    "        random_seed=i)\n",
    "\n",
    "    model.fit(\n",
    "\n",
    "        X_train, y_train,\n",
    "\n",
    "        cat_features=cat_feature_inds)\n",
    "\n",
    "    y_pred_cat += model.predict(X_test)\n",
    "\n",
    "y_pred_cat /= num_ensembles\n",
    "\n",
    "del train\n",
    "\n",
    "del test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "\n",
    "################\n",
    "\n",
    "##    OLS     ##\n",
    "\n",
    "################\n",
    "\n",
    "################\n",
    "\n",
    "np.random.seed(17)\n",
    "\n",
    "random.seed(17)\n",
    "\n",
    "print( \"\\n\\nProcessing data for OLS ...\")\n",
    "\n",
    "print( \"\\nProcessing data for XGBoost ...\")\n",
    "\n",
    "data_path = 'datas/'\n",
    "\n",
    "train = pd.read_csv(data_path+'train.csv',encoding='gb18030')\n",
    "\n",
    "test = pd.read_csv(data_path+'testA.csv',encoding='gb18030')\n",
    "\n",
    "def make_feat_ols(train,test):\n",
    "\n",
    "    train_id = train.id.values.copy()\n",
    "\n",
    "    test_id = test.id.values.copy()\n",
    "\n",
    "    data = pd.concat([train,test])\n",
    "\n",
    "    data['性别'] = data['性别'].map({'男':1,'女':0})\n",
    "\n",
    "    data['体检日期'] = (pd.to_datetime(data['体检日期']) - parse('2016-10-09')).dt.days\n",
    "\n",
    " #   data.drop(['乙肝表面抗原', '乙肝表面抗体', '乙肝e抗原','乙肝e抗体', '乙肝核心抗体'],axis=1)\n",
    "\n",
    "    data.fillna(-1,axis=1,inplace=True)\n",
    "\n",
    "    for c, dtype in zip(data.columns, data.dtypes):\n",
    "\n",
    "        if dtype == np.float64:\n",
    "\n",
    "            data[c] = data[c].astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    train_feat = data[data.id.isin(train_id)]\n",
    "\n",
    "    test_feat = data[data.id.isin(test_id)]\n",
    "\n",
    "    \n",
    "\n",
    "    return train_feat,test_feat\n",
    "\n",
    "df_train,df_test= make_feat_ols(train,test)\n",
    "\n",
    "y = df_train['血糖'].values\n",
    "\n",
    "train = df_train.drop(['id', '血糖'], axis=1)\n",
    "\n",
    "test = df_test.drop(['id','血糖'], axis=1)\n",
    "\n",
    "# shape        \n",
    "\n",
    "print('Shape train: {}\\nShape test: {}'.format(train.shape, test.shape))\n",
    "\n",
    "def MSE(y, ypred):\n",
    "\n",
    "    #logerror=log(Zestimate)−log(SalePrice)\n",
    "\n",
    "    return np.sum([np.square(y[i]-ypred[i])  for i in range(len(y))]) / (2*len(y))\n",
    "\n",
    "exc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] \n",
    "\n",
    "col = [c for c in train.columns if c not in exc]\n",
    "\n",
    "print(\"\\nFitting OLS...\")\n",
    "\n",
    "reg = LinearRegression(n_jobs=-1)\n",
    "\n",
    "reg.fit(train, y); print('fit...')\n",
    "\n",
    "print(MSE(y, reg.predict(train)))\n",
    "\n",
    "train = [];  y = [] #memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "\n",
    "########################\n",
    "\n",
    "##  Combine and Save  ##\n",
    "\n",
    "########################\n",
    "\n",
    "########################\n",
    "\n",
    "##### COMBINE PREDICTIONS\n",
    "\n",
    "FUDGE_FACTOR = 0.9863\n",
    "\n",
    "print( \"\\nCombining XGBoost, LightGBM, NN, and baseline predicitons ...\" )\n",
    "\n",
    "lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT-CAT_WEIGHT \n",
    "\n",
    "lgb_weight0 = lgb_weight / (1 - OLS_WEIGHT)\n",
    "\n",
    "xgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\n",
    "\n",
    "cat_weight0= CAT_WEIGHT / (1 - OLS_WEIGHT)\n",
    "\n",
    "baseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\n",
    "\n",
    "nn_weight0 = NN_WEIGHT / (1 - OLS_WEIGHT)\n",
    "\n",
    "pred0 = 0\n",
    "\n",
    "pred0 += xgb_weight0*xgb_pred\n",
    "\n",
    "pred0 += baseline_weight0*BASELINE_PRED\n",
    "\n",
    "pred0 += lgb_weight0*p_test\n",
    "\n",
    "pred0 += nn_weight0*nn_pred\n",
    "\n",
    "pred0 += cat_weight0*y_pred_cat\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/NN/CAT/baseline predictions:\" )\n",
    "\n",
    "print( pd.DataFrame(pred0).head() )\n",
    "\n",
    "print( \"\\nPredicting with OLS and combining with XGB/LGB/NN/CAT/baseline predicitons: ...\" )\n",
    "\n",
    "  \n",
    "\n",
    "pred = FUDGE_FACTOR * ( OLS_WEIGHT*reg.predict(test) + (1-OLS_WEIGHT)*pred0 )\n",
    "\n",
    "submission = [float(format(x, '.4f')) for x in pred]\n",
    "\n",
    "print('predict...')\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/NN/CAT/baseline/OLS predictions:\" )\n",
    "\n",
    "#print(submission)\n",
    "\n",
    "print(MSE(xgb_pred,submission))\n",
    "\n",
    "submission=pd.DataFrame(submission)\n",
    "\n",
    "##### WRITE THE RESULTS\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print( \"\\nWriting results to disk ...\" )\n",
    "\n",
    "submission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False,header=False)\n",
    "\n",
    "print( \"\\nFinished ...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
